import streamlit as st
import numpy as np
import pandas as pd
from typing import Tuple
import matplotlib.pyplot as plt
import gensim
from gensim.models import Word2Vec
import torch
from torch.utils.data import DataLoader, TensorDataset
import torch.nn.functional as F
import torch.nn as nn
import torchutils as tu
from sklearn.model_selection import train_test_split
from collections import Counter
from torchmetrics.classification import BinaryAccuracy
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModel
import time
import re
import string
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
import joblib
from torch import Tensor

def data_preprocessing(text: str) -> str:
    """preprocessing string: lowercase, removing html-tags, punctuation and stopwords

    Args:
        text (str): input string for preprocessing

    Returns:
        str: preprocessed string
    """    

    text = text.lower()
    text = re.sub('<.*?>', '', text) # html tags
    text = ''.join([c for c in text if c not in string.punctuation])# Remove punctuation
    text = [word for word in text.split() if word not in stop_words] 
    text = ' '.join(text)
    return text

def get_words_by_freq(sorted_words: list, n: int ) -> list:
    return list(filter(lambda x: x[1] > n, sorted_words))

def padding(review_int: list, seq_len: int) -> np.array:
    """Make left-sided padding for input list of tokens

    Args:
        review_int (list): input list of tokens
        seq_len (int): max length of sequence, it len(review_int[i]) > seq_len it will be trimmed, else it will be padded by zeros

    Returns:
        np.array: padded sequences
    """    
    features = np.zeros((len(reviews_int), seq_len), dtype = int)
    for i, review in enumerate(review_int):
        if len(review) <= seq_len:
            zeros = list(np.zeros(seq_len - len(review)))
            new = zeros + review
        else:
            new = review[: seq_len]
        features[i, :] = np.array(new)
            
    return features

choice = st.selectbox('–í—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å', ['Bert', 'LogReg', 'LSTM'])

if choice == 'Bert':
    class MyTinyBERT(torch.nn.Module):
        def __init__(self, num_classes: int = 2, dropout: float = 0.3):
            super().__init__()
            self.bert = AutoModel.from_pretrained("cointegrated/rubert-tiny2")
            
            for param in self.bert.parameters():
                param.requires_grad = False
            for param in self.bert.encoder.layer[-2:].parameters():
                param.requires_grad = True

            hidden_size = self.bert.config.hidden_size
            self.classifier = torch.nn.Sequential(
                torch.nn.Linear(hidden_size, 128),
                torch.nn.ReLU(),
                torch.nn.Dropout(dropout),
                torch.nn.Linear(128, 64),
                torch.nn.ReLU(),
                torch.nn.Dropout(0.2),
                torch.nn.Linear(64, num_classes)
            )
            self.num_classes = num_classes

        def forward(self, batch):
            input_ids = batch['input_ids'].to(self.bert.device)
            attention_mask = batch['attention_mask'].to(self.bert.device)
            bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            cls_output = bert_out.last_hidden_state[:, 0, :]
            logits = self.classifier(cls_output)
            return logits


    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    @st.cache_resource
    def load_model():
        # –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = torch.load("models/ML+LSTM+Bert/bert/my_tinybert_config.pth")
        
        # –°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å
        model = MyTinyBERT(num_classes=config['num_classes'], dropout=config['dropout'])
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–µ—Å–∞
        model.load_state_dict(torch.load("models/ML+LSTM+Bert/bert/my_tinybert_finetuned.pth", map_location=torch.device('cpu')))
        model.eval()
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        tokenizer = AutoTokenizer.from_pretrained("models/ML+LSTM+Bert/bert/tokenuzer")
        
        return model, tokenizer

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    st.title("BERT Text Classifier")
    st.write("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")

    # –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    model, tokenizer = load_model()


    # –ü–æ–ª–µ –≤–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞
    text = st.text_area("–¢–µ–∫—Å—Ç", height=150)


    if text:
        start_time = time.time()  # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å (–≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥)
        inputs = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors="pt")
        with torch.no_grad():
            outputs = model(inputs)
            probabilities = torch.softmax(outputs, dim=1)
            predicted_class = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0][predicted_class].item()

        end_time = time.time()
        inference_time = end_time - start_time

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–ª–∞—Å—Å–æ–≤ –º–µ—Ç–∫–∞–º
        class_labels = {0: "–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π", 1: "–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π"}
        predicted_label = class_labels[predicted_class]  # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–æ–º–µ—Ä –∫–ª–∞—Å—Å–∞ –≤ –º–µ—Ç–∫—É

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –Ω–æ–≤—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        st.write(f"**–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å:** {predicted_label}")
        st.write(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {confidence:.4f}")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º (—Å –ø–æ–¥–ø–∏—Å—è–º–∏)
        st.write("**–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º:**")

        # –°–æ–∑–¥–∞—ë–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–≤
        cols = st.columns(2)

        for i, prob in enumerate(probabilities[0]):
            prob_rounded = round(prob.item(), 4)
            label = class_labels[i]  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–∫—É –≤–º–µ—Å—Ç–æ –Ω–æ–º–µ—Ä–∞ –∫–ª–∞—Å—Å–∞

            # –ö–æ–ª–æ–Ω–∫–∞ 1: —Ç–µ–∫—Å—Ç —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
            with cols[0]:
                st.write(f"{label}")

            # –ö–æ–ª–æ–Ω–∫–∞ 2: –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
            with cols[1]:
                st.progress(prob_rounded)  # –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –æ—Ç 0 –¥–æ 1
                st.caption(f"{prob_rounded:.4f}")  # —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–¥ –±–∞—Ä–æ–º

        # –¢–∞–±–ª–∏—Ü–∞ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è) —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        st.dataframe(
            pd.DataFrame({
                '–ö–ª–∞—Å—Å': [class_labels[i] for i in range(len(probabilities[0]))],  # –∑–∞–º–µ–Ω—è–µ–º –Ω–æ–º–µ—Ä–∞ –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ –º–µ—Ç–∫–∏
                '–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å': [round(p.item(), 4) for p in probabilities[0]]
            }),
            hide_index=True,
            use_container_width=True
        )

        # –í—ã–¥–µ–ª–µ–Ω–∏–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–≥–æ –∫–ª–∞—Å—Å–∞ —Å —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–µ—Ç–∫–æ–π
        max_probs, max_idxs = torch.max(probabilities, dim=1)  # dim=1 ‚Äî –ø–æ —Å—Ç—Ä–æ–∫–∞–º (–∫–ª–∞—Å—Å–∞–º)
        max_prob = torch.max(probabilities[0])
        max_idx = torch.argmax(probabilities[0]).item()
        final_label = class_labels[max_idx]  # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–æ–º–µ—Ä –∫–ª–∞—Å—Å–∞ –≤ –º–µ—Ç–∫—É

        st.success(f"**–§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑:** {final_label} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {max_prob.item():.4f})     –í—Ä–µ–º—è: {inference_time:.4f}")

elif choice == 'LSTM':
    df = pd.read_json('data/healthcare_facilities_reviews.jsonl',lines=True)
    df = df[['content','sentiment']]
    content = df['content'].tolist()
    preprocessed = [data_preprocessing(content) for content in content]
    corpus = [word for text in preprocessed for word in text.split()]
    sorted_words = Counter(corpus).most_common()
    sorted_words = get_words_by_freq(sorted_words, 200)
    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}
    reviews_int = []
    for text in preprocessed:
        r = [vocab_to_int[word] for word in text.split() if vocab_to_int.get(word)]
        reviews_int.append(r)
    w2v_input = []
    for review in preprocessed:
        cur_review = []
        for word in review.split():
            if vocab_to_int.get(word):
                cur_review.append(word)
        w2v_input.append(cur_review)
    VOCAB_SIZE = len(vocab_to_int) + 1  # —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –≤–º–µ—Å—Ç–µ —Å —Ç–æ–∫–µ–Ω–æ–º padding
    EMBEDDING_DIM = 64 # embedding_dim 
    # –û–±—É—á–∏–º Word2Vec
    wv = Word2Vec(
        vector_size=EMBEDDING_DIM # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è —Å–ª–æ–≤–∞
        )
    # –°–Ω–∞—á–∞–ª–∞ word2vec —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å–ª–æ–≤–∞—Ä—å
    wv.build_vocab(w2v_input)
    wv.train(
        corpus_iterable=w2v_input, 
        total_examples=wv.corpus_count, 
        epochs=10
        );
    embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))

    # –ë–µ–∂–∏–º –ø–æ –≤—Å–µ–º —Å–ª–æ–≤–∞–º —Å–ª–æ–≤–∞—Ä—è: –µ—Å–ª–∏ —Å–ª–æ–≤–æ –µ—Å—Ç—å –≤ word2vec, 
    # –¥–æ—Å—Ç–∞–µ–º –µ–≥–æ –≤–µ–∫—Ç–æ—Ä; –µ—Å–ª–∏ —Å–ª–æ–≤–∞ –Ω–µ—Ç, —Ç–æ —Ä–∞—Å–ø–µ—á–∞—Ç—ã–≤–∞–µ–º –µ–≥–æ –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    for word, i in vocab_to_int.items():
        try:
            embedding_vector = wv.wv[word]
            embedding_matrix[i] = embedding_vector
        except KeyError as e:
            pass
            print(f'{e}: word: {word}')
            
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ ‚Äì —ç—Ç–æ—Ç —Å–ª–æ–π –≤ –Ω–∞—à–µ–π —Å–µ—Ç–∏ –æ–±—É—á–∞—Ç—å—Å—è –Ω–µ –±—É–¥–µ—Ç
    embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix))
    padded = padding(review_int=reviews_int, seq_len=64)
    X_train, X_valid, y_train, y_valid = train_test_split(
        np.array(padded),
        pd.get_dummies(
            df['sentiment'], 
            drop_first=True
        ).values.astype('int'), test_size=0.2, random_state=1)
    BATCH_SIZE = 64

    train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))
    valid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))
    train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)
    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE)
    dataiter = iter(train_loader)
    sample_x, sample_y = next(dataiter)
    VOCAB_SIZE = len(vocab_to_int)+1 
    SEQ_LEN = 32
    BATCH_SIZE = 64
    device='cpu'
    HIDDEN_SIZE = 32
    class BahdanauAttention(nn.Module):
        def __init__(self, hidden_size):
            super().__init__()
            self.hidden_size = hidden_size
            self.W = nn.Linear(hidden_size, hidden_size)
            self.V = nn.Linear(hidden_size, 1)
        
        def forward(self, hidden, rnn_outputs):
            """
            Args:
                hidden: (batch_size, hidden_size) - –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                rnn_outputs: (batch_size, seq_len, hidden_size) - –≤—Å–µ –≤—ã—Ö–æ–¥—ã RNN
            """
            # hidden: (batch_size, hidden_size) -> (batch_size, 1, hidden_size)
            hidden = hidden.unsqueeze(1)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ—Ä—ã –≤–Ω–∏–º–∞–Ω–∏—è
            # rnn_outputs: (batch_size, seq_len, hidden_size)
            # hidden: (batch_size, 1, hidden_size)
            
            scores = torch.tanh(self.W(rnn_outputs) + self.W(hidden))
            scores = self.V(scores).squeeze(-1)  # (batch_size, seq_len)
            
            # –í–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è
            attention_weights = F.softmax(scores, dim=1)  # (batch_size, seq_len)
            
            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ - –£–ë–ï–î–ò–¢–ï–°–¨ –ß–¢–û –§–û–†–ú–´ –ü–†–ê–í–ò–õ–¨–ù–´–ï
            # attention_weights: (batch_size, 1, seq_len)
            # rnn_outputs: (batch_size, seq_len, hidden_size)
            context_vector = torch.bmm(attention_weights.unsqueeze(1), rnn_outputs)
            context_vector = context_vector.squeeze(1)  # (batch_size, hidden_size)
            
            return context_vector, attention_weights
    from dataclasses import dataclass
    from typing import Union
    @dataclass
    class ConfigRNN:
        vocab_size: int
        device: str
        n_layers: int
        embedding_dim: int
        hidden_size: int
        seq_len: int
        bidirectional: Union[bool, int]
        embedding_dropout: float
    net_config = ConfigRNN(
        vocab_size=len(vocab_to_int) + 1,
        device="cpu",
        n_layers=2,
        embedding_dim=16,
        hidden_size=32,
        seq_len=SEQ_LEN,
        bidirectional=False,
        embedding_dropout=0.2
    )
    class LSTMnetAttention(nn.Module):
        def __init__(self, rnn_conf=net_config):
            super().__init__()
            self.rnn_conf = rnn_conf
            self.vocab_size = rnn_conf.vocab_size
            self.emb_size = rnn_conf.embedding_dim
            self.hidden_dim = rnn_conf.hidden_size
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –∑–∞—Ö–≤–∞—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏–∫–∏
            self.embedding = nn.Embedding(self.vocab_size, self.emb_size)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LSTM –≤–º–µ—Å—Ç–æ RNN - –ª—É—á—à–µ –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
            self.rnn = nn.LSTM(
                input_size=self.emb_size,
                hidden_size=self.hidden_dim,
                batch_first=True,
                bidirectional=True,  # –í–ö–õ–Æ–ß–ê–ï–ú bidirectional –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                num_layers=2,
                dropout=0.3
            )
            
            # –£–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è
            self.attention = BahdanauAttention(self.hidden_dim * 2)
            
            # –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∫–∞
            self.classifier = nn.Sequential(
                nn.Linear(self.hidden_dim * 2, 64),
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.Linear(64, 32),
                nn.ReLU(), 
                nn.Dropout(0.3),
                nn.Linear(32, 1)
            )

        def forward(self, x):
            # Embedding
            x = self.embedding(x)
            
            # LSTM —Å packing –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            lstm_out, (hidden, cell) = self.rnn(x)
            
            # –î–ª—è bidirectional LSTM –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            if self.rnn.bidirectional:
                last_hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)
            else:
                last_hidden = hidden[-1]
            
            # Attention —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
            context_vector, attention_weights = self.attention(last_hidden, lstm_out)
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
            out = self.classifier(context_vector)
            
            return out, attention_weights  
    model = LSTMnetAttention(net_config)
    criterion = nn.BCEWithLogitsLoss()
    optimizer_rnn = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)
    metric = BinaryAccuracy()


    vocab_to_int = joblib.load("models/ML+LSTM+Bert/lstm/vocab_to_int2.pkl")
    int_to_vocab = {j:i for i, j in vocab_to_int.items()}
    state_dict = torch.load(
            "models/ML+LSTM+Bert/lstm/lstm_sentiment_model.pth", 
            map_location='cpu'
        )
    model.embedding = nn.Embedding(len(vocab_to_int) + 1, 16)

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤
    def load_custom_embedding(model, state_dict):
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π embedding —Å–ª–æ–π
        new_embedding = nn.Embedding(len(vocab_to_int) + 1, 16)
        
        # –ö–æ–ø–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤–µ—Å–∞
        with torch.no_grad():
            # –ö–æ–ø–∏—Ä—É–µ–º –≤–µ—Å–∞ –∏–∑ —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏
            new_embedding.weight[:2979] = state_dict['embedding.weight']
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            nn.init.xavier_uniform_(new_embedding.weight[2979:])
        
        # –ó–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—ã–π embedding
        model.embedding = new_embedding
        
        # –£–¥–∞–ª—è–µ–º embedding.weight –∏–∑ state_dict
        del state_dict['embedding.weight']
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        model.load_state_dict(state_dict, strict=False)

    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é
    load_custom_embedding(model, state_dict)
    model.eval()
    # Streamlit –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    st.title("LSTM Sentiment Analysis - –ü–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∏")
    st.write("–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è:")

    # –ü–æ–ª–µ –≤–≤–æ–¥–∞
    text = st.text_area("–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞:", height=150)

    if text:
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        preprocessed = data_preprocessing(text)
        sequence = [vocab_to_int.get(word, 0) for word in preprocessed.split()]
        
        # Padding
        if len(sequence) < 64:
            sequence.extend([0] * (64 - len(sequence)))
        else:
            sequence = sequence[:64]
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        input_tensor = torch.tensor([sequence], dtype=torch.long)
        with torch.no_grad():
            output, attention_weights = model(input_tensor)
            probability = torch.sigmoid(output).item()
            prediction = "Positive" if probability > 0.5 else "Negative"
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç
        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç:")
        if prediction == "Positive":
            st.success(f"‚úÖ {prediction} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {probability:.4f})")
        else:
            st.error(f"‚ùå {prediction} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {probability:.4f})")
        
        # –î–µ—Ç–∞–ª–∏
        st.write(f"**–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:** {preprocessed}")
        st.write(f"**–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:** {len(sequence)}")

elif choice == 'LogReg':
    from stop_words import get_stop_words
    def get_improved_russian_stopwords():
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å—Ç–æ–ø-—Å–ª–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–π –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"""
        stop_words = set(get_stop_words('russian'))
        
        # –£–î–ê–õ–Ø–ï–ú –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤ (–æ—á–µ–Ω—å –≤–∞–∂–Ω–æ!)
        negative_words_to_keep = {
            '–Ω–µ', '–Ω–µ—Ç', '–Ω–∏', '–Ω–∏–∫–∞–∫', '–Ω–∏–∫–æ–≥–¥–∞', '–Ω–∏—Å–∫–æ–ª—å–∫–æ', '–Ω–∏—á—É—Ç—å',
            '–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç', '–ø–ª–æ—Ö–æ', '—É–∂–∞—Å–Ω–æ', '–∫–æ—à–º–∞—Ä', '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ',
            '—Ç–∞–∫ —Å–µ–±–µ', '–Ω–µ –æ—á–µ–Ω—å', '–Ω–µ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å', '–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è'
        }
        
        # –£–±–∏—Ä–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Å—Ç–æ–ø-—Å–ª–æ–≤
        stop_words = stop_words - negative_words_to_keep
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        additional_stopwords = {
            '—ç—Ç–æ', '–≤–æ—Ç', '–∫–∞–∫', '—Ç–∞–∫', '–∏', '–≤', '–Ω–∞–¥', '–∫', '–¥–æ',
            '–Ω–∞', '–Ω–æ', '–∑–∞', '—Ç–æ', '—Å', '–ª–∏', '–∞', '–≤–æ', '–æ—Ç', '—Å–æ',
            '–¥–ª—è', '–æ', '–∂–µ', '–Ω—É', '–≤—ã', '–±—ã', '—á—Ç–æ', '–∫—Ç–æ', '–æ–Ω', '–æ–Ω–∞'
        }
        
        return stop_words.union(additional_stopwords)
    stop_words = get_improved_russian_stopwords()

    def data_preprocessing(text: str) -> str:
        """preprocessing string: lowercase, removing html-tags, punctuation, 
                                stopwords, digits

        Args:
            text (str): input string for preprocessing

        Returns:
            str: preprocessed string
        """    

        text = text.lower()
        text = re.sub('<.*?>', '', text) # html tags
        text = ''.join([c for c in text if c not in string.punctuation])# Remove punctuation
        text = ' '.join([word for word in text.split() if word not in stop_words])
        text = ' '.join([word for word in text.split() if not word.isdigit()]) 
        return text

    def get_words_by_freq(sorted_words: list[tuple[str, int]], n: int = 10) -> list:
        return list(filter(lambda x: x[1] > n, sorted_words))

    def padding(review_int: list, seq_len: int) -> np.array: # type: ignore
        """Make left-sided padding for input list of tokens

        Args:
            review_int (list): input list of tokens
            seq_len (int): max length of sequence, it len(review_int[i]) > seq_len it will be trimmed, else it will be padded by zeros

        Returns:
            np.array: padded sequences
        """    
        features = np.zeros((len(review_int), seq_len), dtype = int)
        for i, review in enumerate(review_int):
            if len(review) <= seq_len:
                zeros = list(np.zeros(seq_len - len(review)))
                new = zeros + review
            else:
                new = review[: seq_len]
            features[i, :] = np.array(new)
                
        return features

    def preprocess_single_string(
        input_string: str, 
        seq_len: int, 
        vocab_to_int: dict,
        verbose : bool = False
        ):
        """Function for all preprocessing steps on a single string

        Args:
            input_string (str): input single string for preprocessing
            seq_len (int): max length of sequence, it len(review_int[i]) > seq_len it will be trimmed, else it will be padded by zeros
            vocab_to_int (dict, optional): word corpus {'word' : int index}. Defaults to vocab_to_int.

        Returns:
            list: preprocessed string
        """    

        preprocessed_string = data_preprocessing(input_string)
        result_list = []
        for word in preprocessed_string.split():
            try: 
                result_list.append(vocab_to_int[word])
            except KeyError as e:
                if verbose:
                    print(f'{e}: not in dictionary!')
                pass
        result_padded = padding([result_list], seq_len)[0]

        return Tensor(result_padded)
    import pandas as pd

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—à —Ñ–∞–π–ª —Å –æ—Ç–∑—ã–≤–∞–º–∏
    df = pd.read_json('data/healthcare_facilities_reviews.jsonl',lines=True)
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.preprocessing import LabelEncoder

    vectorizer = CountVectorizer(max_features=5000)  # –±–µ—Ä–µ–º 5000 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤

    X = vectorizer.fit_transform(df['content'])
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(df['sentiment'])

    print(f"–ü–æ–ª—É—á–∏–ª–∏ –º–∞—Ç—Ä–∏—Ü—É: {X.shape}")
    stop_words = list(stop_words)
    from sklearn.feature_extraction.text import TfidfVectorizer
    tfidf = TfidfVectorizer(max_features=5000, stop_words=stop_words)
    X = tfidf.fit_transform(df['content'])
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    from sklearn.linear_model import LogisticRegression
    lr_model = LogisticRegression()
    import matplotlib.pyplot as plt
    import numpy as np
    from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                            f1_score, confusion_matrix, classification_report, 
                            roc_curve, auc, precision_recall_curve)
    from sklearn.model_selection import learning_curve
    import joblib
    vectorizer = joblib.load("models/ML+LSTM+Bert/lm/logreg_model_vectorizer.pkl")
    lr_model = joblib.load("models/ML+LSTM+Bert/lm/logreg_model_model.pkl")
    label_encoder = joblib.load("models/ML+LSTM+Bert/lm/logreg_model_label_encoder.pkl")
    st.title("üìä –ê–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤ –æ –ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞—Ö (LogReg)")
    st.write("–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è:")

    # –ü–æ–ª–µ –≤–≤–æ–¥–∞
    text = st.text_area("–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞:", height=150, 
                    placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: '–û—Ç–ª–∏—á–Ω–∞—è –ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞, –≤—Ä–∞—á–∏ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–µ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ...'")

    if st.button("üéØ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å") and text:
        if vectorizer is not None and lr_model is not None:
            with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ç–∑—ã–≤..."):
                # 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
                processed_text = data_preprocessing(text)
                
                # 2. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ TF-IDF
                text_vector = vectorizer.transform([processed_text])
                
                # 3. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                prediction = lr_model.predict(text_vector)[0]
                probability = lr_model.predict_proba(text_vector)[0]
                
                # 4. –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞
                if label_encoder is not None:
                    sentiment = label_encoder.inverse_transform([prediction])[0]
                else:
                    sentiment = "Positive" if prediction == 1 else "Negative"
                
                # 5. –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                st.subheader("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:")
                
                if prediction == 1:
                    st.success(f"‚úÖ {sentiment} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {probability[prediction]:.2%})")
                else:
                    st.error(f"‚ùå {sentiment} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {probability[prediction]:.2%})")
                
                # 6. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                st.subheader("üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤:")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    prob_negative = probability[0] if prediction == 0 else 1 - probability[1]
                    st.write("üî¥ Negative")
                    st.progress(prob_negative)
                    st.write(f"{prob_negative:.4f}")
                
                with col2:
                    prob_positive = probability[1] if prediction == 1 else 1 - probability[0]
                    st.write("üü¢ Positive")
                    st.progress(prob_positive)
                    st.write(f"{prob_positive:.4f}")
                
                # 7. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                st.subheader("‚ÑπÔ∏è –î–µ—Ç–∞–ª–∏:")
                st.write(f"**–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:** {processed_text}")
                st.write(f"**–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞:** {text_vector.shape[1]} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                
        else:
            st.error("‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º.")
