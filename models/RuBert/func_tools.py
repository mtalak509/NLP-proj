import matplotlib.pyplot as plt
import numpy as np
import torch

def get_toxic_classification_with_attention(text: str, model, tokenizer, device='cpu'):
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º attention –≤–µ—Å–æ–≤
    
    Returns:
        dict: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ attention –≤–µ—Å–∞
    """
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokenized = tokenizer(
        text,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors='pt'
    )
    
    model.to(device)
    model.eval()
    
    with torch.no_grad():
        input_ids = tokenized['input_ids'].to(device)
        attention_mask = tokenized['attention_mask'].to(device)
        
        # –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è attention –≤–µ—Å–æ–≤ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å output_attentions=True
        outputs = model(
            input_ids=input_ids, 
            attention_mask=attention_mask,
            output_attentions=True  # –≤–∞–∂–Ω–æ!
        )
        
        if hasattr(outputs, 'logits'):
            logits = outputs.logits
        else:
            logits = outputs
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = torch.argmax(logits, dim=1).item()
        probability = torch.softmax(logits, dim=1)[0][prediction].item()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º attention –≤–µ—Å–∞
        attentions = outputs.attentions  # tuple —Å attention –≤—Å–µ—Ö —Å–ª–æ–µ–≤
        
        # –ë–µ—Ä–µ–º attention –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
        last_layer_attention = attentions[-1]  # [batch_size, num_heads, seq_len, seq_len]
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤—Å–µ–º –≥–æ–ª–æ–≤–∞–º –≤–Ω–∏–º–∞–Ω–∏—è
        avg_attention = last_layer_attention.mean(dim=1)  # [batch_size, seq_len, seq_len]
        
        # –ë–µ—Ä–µ–º attention –¥–ª—è [CLS] —Ç–æ–∫–µ–Ω–∞ (–∫–∞–∫ –æ–Ω "—Å–º–æ—Ç—Ä–∏—Ç" –Ω–∞ –¥—Ä—É–≥–∏–µ —Ç–æ–∫–µ–Ω—ã)
        cls_attention = avg_attention[0, 0, :]  # [seq_len]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã
        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
        
        return {
            'prediction': prediction,
            'probability': probability,
            'class_name': 'toxic' if prediction == 1 else 'non-toxic',
            'tokens': tokens,
            'attention_weights': cls_attention.cpu().numpy()
        }
    


def merge_subtokens(tokens, attention_weights):
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—É–±—Ç–æ–∫–µ–Ω—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ª–æ–≤–∞ –∏ —Å—É–º–º–∏—Ä—É–µ—Ç –∏—Ö attention –≤–µ—Å–∞
    
    Args:
        tokens: —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        attention_weights: —Å–ø–∏—Å–æ–∫ attention –≤–µ—Å–æ–≤
    
    Returns:
        tuple: (—Å–ª–æ–≤–∞, –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ attention –≤–µ—Å–∞)
    """
    words = []
    merged_attention = []
    current_word = ""
    current_attention = 0.0
    count = 0
    
    for token, attention in zip(tokens, attention_weights):
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        if token in ['[CLS]', '[SEP]', '[PAD]']:
            if current_word:
                words.append(current_word)
                merged_attention.append(current_attention / count if count > 0 else current_attention)
                current_word = ""
                current_attention = 0.0
                count = 0
            continue
        
        # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å ## - —ç—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–æ–≤–∞
        if token.startswith('##'):
            current_word += token[2:]  # —É–±–∏—Ä–∞–µ–º ##
            current_attention += attention
            count += 1
        else:
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—É—â–µ–µ —Å–ª–æ–≤–æ, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ
            if current_word:
                words.append(current_word)
                merged_attention.append(current_attention / count if count > 0 else current_attention)
            
            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤–æ–µ —Å–ª–æ–≤–æ
            current_word = token
            current_attention = attention
            count = 1
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ
    if current_word:
        words.append(current_word)
        merged_attention.append(current_attention / count if count > 0 else current_attention)
    
    return words, merged_attention




def visualize_attention_merged(text: str, model, tokenizer, device='cpu'):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è attention –≤–µ—Å–æ–≤ —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
    """
    result = get_toxic_classification_with_attention(text, model, tokenizer, device)
    
    tokens = result['tokens']
    attention_weights = result['attention_weights']
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—É–±—Ç–æ–∫–µ–Ω—ã –≤ —Å–ª–æ–≤–∞
    words, merged_attention = merge_subtokens(tokens, attention_weights)
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
    fig = plt.figure(figsize=(12, 6))
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç–∞ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
    mean_attention = np.mean(merged_attention)
    colors = ['red' if w > mean_attention else 'blue' for w in merged_attention]
    
    bars = plt.bar(range(len(words)), merged_attention, color=colors, alpha=0.7)
    plt.xticks(range(len(words)), words, rotation=45, ha='right', fontsize=10)
    plt.title(f'Attention Weights - Prediction: {result["class_name"].upper()} (confidence: {result["probability"]:.3f})', 
              fontsize=14, fontweight='bold', pad=20)
    plt.ylabel('Attention Weight', fontsize=12)
    plt.grid(True, alpha=0.3, axis='y')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, value in zip(bars, merged_attention):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                f'{value:.3f}', ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    plt.show()
    
    # # –í—ã–≤–æ–¥–∏–º —Å–ª–æ–≤–∞ —Å attention –≤–µ—Å–∞–º–∏
    # print("–°–ª–æ–≤–∞ —Å attention –≤–µ—Å–∞–º–∏:")
    # print("-" * 40)
    # for word, weight in zip(words, merged_attention):
    #     importance = "üî¥ –í–ê–ñ–ù–û" if weight > mean_attention else "üîµ –Ω–æ—Ä–º–∞–ª—å–Ω–æ"
    #     print(f"{word:15} {weight:.4f} {importance}")
    
    # print(f"\n–°—Ä–µ–¥–Ω–∏–π attention: {mean_attention:.4f}")
    # print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {result['class_name']} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {result['probability']:.3f})")
    
    # # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    # result['merged_words'] = words
    # result['merged_attention'] = merged_attention
    
    return result, fig